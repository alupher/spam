\documentclass[preprint]{acm_proc_article-sp}
%\documentclass[preprint]{sig-alternate}
\usepackage{url}
\usepackage{graphicx,subfigure}
\usepackage{xspace}
\usepackage{multirow}

\newcommand{\ie}{{\em i.e.,}~}
\newcommand{\eg}{{\em e.g.,}~}

\setcounter{page}{1}
\pagenumbering{arabic}

\newenvironment{denseitemize}{
\begin{itemize}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newcommand{\eat}[1]{}


\begin{document}

\title{Detecting Spam on Social Networking Sites}

\numberofauthors{3}
\author{
Antonio Lupher,
Cliff Engle,
Reynold Xin\\\\
\texttt{\{alupher, cengle, rxin\}@cs.berkeley.edu}
}


\maketitle
\begin{abstract}
Social networking sites (SNSs) see a variety of spam and scams targeted at 
their users. In contrast to the more limited amounts of secondary information 
available when analyzing email spam, spam on SNSs is accompanied by a wealth 
of data on the sender, which can be used to build more accurate detection mechanisms. 
We analyze public and private data from a popular social network in order to 
gain insight into the various features spam messages and the accompanying user 
accounts and profiles available to site operators. By examining the ....
\end{abstract}


\maketitle

\section{Introduction}
% Modified from project proposal

Social networks of any significant size see constant spam, scams
and phishing attacks. The nature of these attacks can be quite diverse
and difficult detect. Marketers can spam members with unwanted
advertisements, fraudsters lure users with advance fee frauds and
other confidence tricks, while others may attempt to steal user
information by directing users to external phishing pages. 
Sites with global reach see communication among its members in a 
number foreign languages with varying levels
of ability. This means that much benign content shares characteristics
like misspellings, awkward phrases, etc. that might have made certain
types of common frauds and spam more easy to distinguish on US-based
(or English-language) sites.
% focus on extra impact of non-public features vs. public (bag-of-words)

\subsection{Approach}

Email spam and techniques with which to combat it has been studied 
extensively. Like
This paper examines in detail the types of malicious and benign
content that are encountered on social networks by analyzing
experimental data available from InterPals, an international social network
for cultural exchange and language practice. For example, the site
attracts a wide variety of financial scams, ranging from Nigerian
"419" scams to romance scams. Another prevalent problem is spam with
links to third-party websites, directing users to various porn/webcam
sites, phishing sites or various untrustworthy online marketplaces.

We examine various methods of detecting and preventing abuse
on the site, including those measures that have already been taken
(e.g. various heuristics including IP/location anomaly detection,
frequency capping, duplicate account detection, etc.). We then analyze 
message and user account data points to try to differentiate legitimate users
from malicious ones. By mining this data, we extract features 
to build and evaluate classifiers that can detect unwanted behavior 
programmatically. The large volume of data available to us, although we do 
not use all of it, provides a unique perspective both on the types of
malicious content that exist on such sites as well as on the
effectiveness of classifier-based approaches to identifying
these activities.

We investigated several machine learning techniques to automatically 
detect spam and scams in the private message data, and chose to use 
train and evaluate Na\"ive Bayes, Linear Regression, and 
Support Vector Machine (SVM) classifiers. Our implementations used 
a variety of tools, including Matlab, ScalaNLP, LIBSVM, Lucene and Spark, 
an in-memory distributed computing framework designed for machine 
learning and iterative computation.


\subsection{Data sets}

We enjoyed unrestricted access to the data of InterPals.net, a SNS with 
over 1.3 million active members. This data includes a corpus of over a 
100 million private messages and another 2 million messages that have been 
labeled as spam by users. Other data includes 40 million or so "wall" 
comments, 5 million photos, and 8 million photo comments. 


\section{Related Work} 

The rapid growth of social media has made Social Networking Services (SNSs) increasingly attractive targets for spam and fraud, leading to a proliferation of sophisticated attacks. This trend is reflected in recent research, as papers have focused on identifying and classifying the various types of social media spam. Many of these studies employ techniques previously used to combat conventional email and web spam. SNSs also provide opportunities to take advantage of user reputation and other social graph-dependent features to improve classification. Nevertheless, most research has been carried out on publicly-available data from SNSs, making it difficult up until now to measure the effect of private user data on algorithms for detecting site misuse.


\subsection{Social Spam Features}

Heymann et al. \cite{heymann} survey the field of spam on SNSs, identifying several common approaches. Identification-based approaches identify spam to train classifiers based on labels submitted by users or trusted moderators. Rank-based approaches demote visibility of questionable content, while interface-based approaches apply policies to prevent unwanted behavior. This work groups classification-based approaches with detection, although classifiers can be used in conjunction with user information to prevent spam before it happens.

A number of researchers have focused on collecting, identifying features and classifying various genres of spam on social networks. Zinman and Donath \cite{zinman} extract bundles of profile-based and comment-based features from MySpace profiles, but the relatively poor performance of their classifier highlights the difficulties in manual classification social network spam. Several studies take the approach of baiting spammers with social ``honeypots", profiles created with the sole intent of attracting spam.\cite{stringhini, lee} They then use the data collected to train classifiers with features including friend request rate and ratios of URLs to text. Webb et al. \cite{webb} use the honeypot approach as well and provide examples of various types of spammers, the typical demographics of their profiles as well as the web pages that they tend to advertise. 

Gao et al. \cite{gao} look at Facebook wall posts, analyzing temporal properties, URL characteristics, post ratios and other features of malicious accounts. They also pinpoint various spam ``campaigns" based on products advertised in a given time frame. They note that spam on Facebook often exhibits burstiness and is mainly sent from compromised accounts. Benevenuto et al. \cite{benevenuto} identify attributes of spam on video SNSs and use a Support Vector Machine (SVM) for classification.

Not all undesirable content on SNSs is necessarily spam or a scam. SNSs and online communities witness inappropriate user behavior, where users post offensive and harassing content. Yin et al. \cite{yin} combine sentiment analysis and profanity word lists with contextual features to identify harassment on datasets from Slashdot and MySpace. Other work looks at SNSs as platforms to collect data about users in order to aid direct attacks on the user's computers or to compromise a large number of accounts. \cite{patsakis, huber}

\subsection{Social Spam Detection Systems}

SocialSpamGuard \cite{jin} is a social media spam detection system that analyzes text and image features of social media posts. The demo system uses GAD clustering \cite{jingad} for sampling spam and ham posts, then trains a classifier with text and image features. However, the system is built on top of Facebook features that are publicly accessible and thus cannot make use of sensitive user data (\eg IP addresses) to increase its effectiveness. 

De Wang et al. \cite{wang} propose a cross-site spam detection framework to share spam data across all social networking sites, building classifiers to identify spam in profiles, messages and web pages. This multi-pronged approach lends itself to associative classification, in which, for example, a message would be classified as spam if it contained a link to a web page that had a high probability of being spam. Unfortunately, the differing characteristics of various social networks \eg the length of messages in Facebook vs. Twitter, can reduce the benefits of sharing spam corpora across diverse sites.

Facebook \cite{stein} provides an overview of their ``immune system" defences against phishing, fraud and spam. The system is composed of classifier services, an ML-derived Feature Extraction Language (FXL), feature loops to aggregate and prepare features for classification and a policy engine to take action on suspected misuse. While the discussion remains high-level and includes few implemention particulars, it does include significant detail on the various types and characteristics of undesirable activity on the site, including fake profiles, harassment, compromised accounts, malware and spam. 

In contrast to research that focuses on dynamically detecting spam based on user activity, Irani et al. \cite{irani} show that static features associated with user signups on MySpace are enough to train an effective social spam classifier. They note that C4.5 decision tree algorithms provide better performance than Na\"ive Bayes in this case. As in other works, this only examines publicly available profile information collected by social honeypots. Private data collected on users including browser features, IP addresses and geographic location would conceivably improve classifier performance substantially.

Bosma et al. \cite{bosma} explore user-generated spam reports as a tool for building an unsupervised spam detection framework for SNSs. Their approach counts the number of spam reports against a suspected spammer and adds weight to reports based on user reputation. Determining reputation and trustworthiness of users in social networks has been well studied \cite{bian, guha, zhang} and appears to be a promising addition to social spam classification. The framework uses a Bayesian classifier and links messages with similar content, but does not take into account other features. Nevertheless, this is one of the few studies to test its framework on non-public data, including private messages, spam reports and user profiles from a large Dutch social networking site.

\subsection{Spam Email \& Web}

Much work has been done on protecting traditional email systems from spam. Blanzieri \cite{blanzieri} offers a comprehensive overview of machine learning techniques that can be applied to email filtering. Hao et al. \cite{hao} describe a reputation engine based on lightweight features such as geographic distance between sender and receiver, geolocation anomalies and diurnal patterns. While the target was conventional spam, these and similar features are applicable to spam on SNSs as well.

Whittaker et al. \cite{whittaker} describe a scalable phishing machine learning classifier and blacklisting system with high accuracy. Since a considerable amount of social media spam includes links to phishing sites, being able to detect them is critical. Along similar lines, Monarch \cite{thomas} is a system that provides scalable real-time detection of URLs that point to spam web pages as determined by URL features, page content and hosting properties of the target domain.

Blog comment spam have also attracted considerable attention from researchers who have applied machine learning \cite{kolari, nag} and NLP \cite{mishne} techniques to the problem. Likewise, Markines et al. \cite{markines} apply similar techniques to spam on social bookmarking sites.

\subsection{Machine Learning and Data Mining}

Many of the data mining algorithms used to detect spam and patterns of misuse on SNSs are designed with the assumption that the data and the classifier are independent. However, in the case of spam, fraud and other malicious content, users will often modify their behavior to evade detection, leading to degraded classifier performance and the need to re-train classifiers frequently. Several researchers tackle this adversarial problem. Dalvi et al. \cite{dalvi} offer a modified Na\"ive Bayes classifier to detect and reclassify data taking into account the optimal modification strategy that an adversary could choose. Lowd and Meek \cite{lowd} provide a framework for reverse engineering a classifier to determine whether an adversary can efficiently learn enough about a classifier to effectively defeat it.


\section{Data set}

For this project, we had unlimited access to data from InterPals, a SNS for users who wish to communicate with others from other countries, whether for language practice, cultural exchange or friendship. Users sign up by completing a registration form with information about themselves, including age, sex and location. After registering, users can expand their profile page to include descriptions, interests, languages they speak, etc. and upload photographs. After clicking on a link sent to their email address, a user can begin to interact with others on the site via private message, public ``wall" posts, comments on photos and on a bulletin board system.

\subsection{Current Anti-Spam Measures}

Currently, the site combats spam and other Terms of Service violations through volunteer moderators. Users can report content, including private messages, profiles and photographs to moderators, using a form that includes a drop-down menu of pre-selected reasons with the option to add a more detailed message in a text field. Likewise, the message interface allows a user to report a private message spam with a single click. Moderators have access to a queue of these reports. In addition to the material being reported, moderators are able to make decisions based on the data from the reported user's account, including outgoing private messages, IP addresses, as well as a list of other users who have logged in from the same computer (determined via IP address as well as by browser fingerprints). Moderators can then decide to delete the user, send a warning, or clear reports on a user. All moderator actions require them to annotate their decision with a brief log message. When deleting a user, moderators have the option of flagging the reported message (if there is one) as spam, or of flagging all of the user's outgoing private messages as spam.

Other anti-spam measures include widespread use of CAPTCHAs across the site and frequency caps. Short-window frequency caps are in place for all users, limiting the number of messages that can be sent per short time interval (1, 5 and 10 minutes). New users are also subject to a per-day cap on the number of unique users with whom they are able to initiate contact. 

Users are able to filter messages by age, sex, continent and country. At this time, 

\subsection{Spam Set}

At the beginning of this research project, slightly two million messages had been flagged by moderators as spam. We extracted the contents of exactly two million spam-labeled messages from accounts deleted by moderators between October 2011 and March 2012. As moderators can flag a user's entire list of outgoing messages as spam on deletion, the earliest sent dates of some messages in our data set begin in May 2010. 

InterPals stores private messages and user account data in a number of separate MySQL tables. Account information for deleted users is stored for six months, so account details for all spam and ham accounts was available. Extracting the data of interest to this project required dumping the contents from a query with multiple joins and aggregations across 8 tables with a combined size of 154 GB. To minimize the impact on the load of the site's production database instances, we took an LVM snapshot of slave instance and cloned the database to a spare server. In addition to using SQL to extract the data, we wrote Perl scripts to clean the data, tokenize messages and prepare data for further processing. The methodology section (X) offers more details on precisely which items of data were collected and why.


\subsection{Ham Set}

Unlike spam messages, we did not have access to a comparable corpus of human-labeled ham messages. To simplify the labeling of ham messages, we made the assumption that messages remaining in the inboxes of active users after a period of several months would most likely not be spam. Consequently, we extracted a working set of 2 million ham messages that were sent in the beginning of January, 2012 and still existed in the recipient's inbox. To reduce the possibility of collecting messages sent by uncaught spammers to inactive or dormant users, we selected messages only from users and to recipients who had logged in within the last two weeks. As in the case of spam, we collected two million messages with associated account data and merged them into text files for feature extraction.

\subsection{Categories of Spam}

In the course of collecting the data, we observed a number of different types of undesirable messages. Our ongoing observation of spam on the InterPals website provided direct intuition into the major classes of unwanted behavior that we classified as spam for this project. We noted the following broad categories:

\textbf{Advance Fee Fraud} We observed this scam to come in various flavors.  (inheritance, lottery,  etc)
\textbf{Romance Scams:} These 
\textbf{Sexual Solicitation}
\textbf{Porn Ads}
\textbf{Advertising External Sites}
\textbf{Webcam Ads}
\textbf{Money Muling}
\textbf{Begging \& Gift Requests}
\textbf{Business Proposals}


This categorizations are based only on observation. Analyzing the relevant terms in log comments entered by moderators when deleting spammer accounts provides the following breakdown:


\section{Methodology}

We chose to focus on spam in private messages for this paper, given that messages account for 
a majority of user spam reports. In addition to using the bag-of-words representation of 
the message content, we aimed to identify a subset of additional, relevant ``expert" features 
based on public and non-public message and account information that would augment the classifier's 
accuracy. We first extracted a number of fields that we expected might improve classification. 

To choose a sample of relevant features, we computed statistics and generated histograms on 
the extracted features, comparing the ham and spam corpora. These statistics were generated 
using SQL on a table created from the merged and cleaned data generated in the process described above.

\subsection{Message Features}

\textbf{Message body \& subject:} We extracted both the message body and subject from 
each spam and ham message. The cleaning script generated two additional features based 
on this. First, we counted the frequency of non-standard punctuation (we noticed that many 
spam messages would, for example, put spaces before commas, periods and quotation marks, 
and omit spaces after these characters). Second, we calculated the ratio of uppercase to 
lowercase letters in the text, after observing relatively high amounts of uppercase text in spam messages.
%% add numbers

\textbf{Recipient age, sex and country:} These fields were the age, sex and ISO 3166-1 alpha-2 code of the message 
recipient, as listed on their profile. Unfortunately, due to data collection issues, this data was only available 
for the most recent 11,000 spam messages. While the male-to-female ratio of the recipients was very close to that 
of non-spam recipients (both were 47\% male to 53\% female), we found that the age of recipients was typically higher.

\textbf{Recipient replied:} This boolean value indicates whether the recipient replied to the message. We saw 
that the mean reply rate for ham messages was 81.77\% (SD: 38.61), while it was only 0.81\% (SD: 8.97) for spam messages. 

\subsection{Sender Account Features}

We chose message and user account features to analyze primarily by the amount of data that we had for each them, while making sure to include the most vital account information. Table \ref{tab:feats} provides averages on a number of the account statistics that we examined. 
\begin{table}
\begin{tabular}{|l|l|r|r|r|}
    \hline
    \textbf{Feature} & \textbf{Label} & \textbf{Mean} & \textbf{Median} & \textbf{Std. Dev.}\\
    \hline
    \multirow{2}{*}{Unique IPs} & Spam & 11.82 & 2 & 52.52\\
    & Ham & 175.51 & 64 & 322.19\\
    \hline
    \multirow{2}{*}{Sender Age} & Spam & 33.52 & 31 & 11.09\\
    & Ham & 24.86 & 22 & 10.30\\
    \hline
    \multirow{2}{*}{Photos} & Spam & 2.09 & 1 & 8.04\\
    & Ham & 15.69 & 6 & 43.40\\
    \hline
    \multirow{2}{*}{Photo Albums} & Spam & 0.91 & 1 & 0.56\\
    & Ham & 1.89 & 1 & 2.43\\
    \hline
    \multirow{2}{*}{Friends} & Spam & 11.82 & 0 & 52.52\\
    & Ham & 23.26 & 10 & 58.91\\
    \hline
    \multirow{2}{*}{Recipient Age} & Spam & 39.98 & 41 & 12.69\\
    & Ham & 25.02 & 22 & 10.70\\
    \hline
    \multirow{2}{*}{Birth Day} & Spam & 12.31 & 10 & 8.52\\
    & Ham & 15.01 & 15 & 8.92\\
    \hline
    \multirow{2}{*}{Birth Month} & Spam & 5.85 & 5 & 3.33\\
    & Ham & 6.38 & 6 & 3.48\\
    \hline
    \multirow{2}{*}{Profile Desc. Len.} & Spam & 267.30 & 87 & 483.19\\
    & Ham & 439.33 & 267 & 613.26\\
    \hline
    \multirow{2}{*}{\% Fields Completed} & Spam & 22.15 & 14.00 & 30.75\\
    & Ham & 79.29 & 86.00 & 32.85\\
    \hline
\end{tabular}
\caption{Feature statistics}
\label{tab:feats}
\end{table}

\textbf{Sender Country:} The distribution of countries as stated by users on their profile and as 
revealed by their IP address differed remarkably between the spam and ham user groups. 
48\% of spam users claimed to be from the United States, with the next most popular 
countries being the UK, Ghana, Senegal, Germany and Canada taking combined 25\%. 
In contrast, only 15\% ham users claimed to be from the United States, with Russia, 
Korea, UK, France and Germany combining for 24\%. This distribution of profile countries can 
be seen in Figure \ref{fig:cprof}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/country-prof.pdf}
    \caption{Country as stated on user profile.}
    \label{fig:cprof}
\end{figure}

Users are free to choose any country on their profile. We record each IP address from which a 
user logs in to the site. By examining the distribution of countries (using the MaxMind GeoIP 
database) associated with the unique IP addresses (Figure \ref{fig:cip}), we see that the top 
countries for spammers are significantly different, while the top ham countries are 
virtually unchanged. Furthermore, only 30\% of the IP addresses of spam users who 
claimed to be from the United States actually mapped back to a US-based ISP, with a combined 
46\% indicating a Ghana, Nigeria and Senegal ISP (Figure \ref{fig:usaspam}). This contrasts 
with the 73\% that US-based IP addresses comprised for ham users whose profiles stated that 
they were in the United States (Figure \ref{fig:usaham}).

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/country-ip.pdf}
    \caption{Country detected by IP address using MaxMind GeoIP database.}
    \label{fig:cip}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ips-usa-spam.pdf}
    \caption{Distribution of IPs by country for spammers with profiles stating a USA location.}
    \label{fig:usaspam}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/ips-usa-ham.pdf}
    \caption{Distribution of IPs by country for ham users with profiles stating a USA location.}
    \label{fig:usaham}
\end{figure}

\textbf{Sender IPs:} Spammers in general had a smaller number of unique IP addresses associated with them than ham users, as per figure \ref{fig:uniqip}. The mean for spammers was 11.82 with a standard deviation of 52.52 and a median of 2, whereas ham users saw a mean of 175.51, a standard deviation of 322.19 and median of 64. This is likely due to their comparatively low length of time on the site and the fact that many legitimate users are behind NATs that yield a high number of dynamic IP addresses. One possibility for future research is to investigate the ratio of unique IPs for users as a function of the time they have been registered. Likewise, examining unique /16 network blocks instead of unique IP addresses could mitigate the influence of NATs.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/unique-ips.pdf}
    \caption{Number of unique IP addresses associated with user account.}
    \label{fig:uniqip}
\end{figure}

\textbf{Sender Email:} Users must provide email addresses upon signup. The site does not allow users to contact other 
members before verifying that the email address provided is valid, via an account activation link sent to the signup 
address. Subsequent changes to a user's email address on file require a similar confirmation process. Thus, we can be certain 
that the email accounts that we extracted were in use by the senders, at least at the time of registration. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/email.pdf}
    \caption{Domain name of email provided at signup.}
    \label{fig:email}
\end{figure}

In the data processing step, we extracted the domain name from each email address. We found that ham user email domains 
were distributed across top email providers in similar proportions (Hotmail, Yahoo and Gmail accounted for 22\%, 17\% and 17\% 
respectively). For spam user accounts, Hotmail, Yahoo and Gmail accounted for 7\%, 72\% and 6\% respectively. This striking 
predominance of Yahoo email accounts among spammers can be seen in figure \ref{fig:email}. While we have no explanation for 
this, figure \cite{fig:ysmap} shows that Ghana and Nigeria alone account for 38\% of the IP addresses associated with these 
Yahoo accounts. This popularity of Yahoo among West African users has been noted before\cite{burrell}, and appears to be 
due to the early penetration of Yahoo services in the region.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/yahoo-spam.pdf}
    \caption{IP distribution for scam accounts with Yahoo email addresses.}
    \label{fig:yspam}
\end{figure}

\textbf{Sender \& Recipient Age:} The age indicated on their profile tended be higher for spam user than 
for ham users, with median ages of 31 and 22 respectively (Figure \ref{fig:sendage}). Likewise, the 
age of message recipients was higher for spam recipients that for recipients of ham messages, with 
median ages of 41 and 22 (Figure \ref{fig:recipage}). The sample size for spam recipient age 
was only 11,000 messages due to late data collection. We posit that the higher ages indicated on spam profiles, 
mirrored in recipient ages, reflects targeting of older users who are more likely to be financially 
stable. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/recip-age.pdf}
    \caption{Age of message recipient as stated on user profile. Note that 2 million message spam sample 
        had only 11,000 messages with this information.}
    \label{fig:recipage}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/sender-age.pdf}
    \caption{Age of message sender as stated on user profile. }
    \label{fig:sendage}
\end{figure}

\textbf{Sender Birthday:} We analyzed the birthdays and birth months of ham and spam users and noticed that spam users 
were more likely to have birthdays early in the month, and birth months early in the year. This is illustrated in 
Figures \ref{fig:day} and \ref{fig:month}. Given the drop-down select menus on the web site's registration form, reproduced 
Figure \ref{cite:drop}, it seems likely that this is due to an unwillingness on the part of spam users to 
scroll down to lower options (as well as, perhaps, a hesitance to disclose real birthdates). A similar trend is 
observed in the countries with the most discrepancies between IP-detected countries and countries stated on spam 
user profiles, with Afghanistan and Albania comprising 19\%, despite their relatively low representation on the 
site. 

figure \ref{fig:day}
figure \ref{fig:month}
figure \ref{fig:drop}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/dob-day.pdf}
    \caption{Date of month from birthday submitted by user at signup.}
    \label{fig:day}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/dob-month.pdf}
    \caption{Month of birthday as submitted by user at signup.}
    \label{fig:month}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/dropdown.png}
    \caption{Drop-down select menu on signup form.}
    \label{fig:drop}
\end{figure}



\textbf{Extra Profile Features:} We found that the number of photos that user has uploaded is also indicative of their spam or ham reputation. Spammers 

\textbf{Sender Age:}

\textbf{Sender Name:}


\textbf{Username, Name:} While we collected these fields, we did not end up processing or analyzing them further. That said, we saw considerable repetition in the usernames of scammers and it seems that analyzing the substrings in these fields could yield useful features.

\textbf{Sender Sex:}

\subsection{Mixed Features}

\textbf{Account Lifetime.}

\textbf{IP Mismatch.} This 


\section{Classification}

Our goal in classification was not only to build an effective classifier for spam detection, but also to see how the presence of ``expert features", or features based on account and message metadata, could affect classifier accuracy. To this end, with of each of the classifiers we chose to implement, we evaluated performance on a bag-of-words representation of message data as well as a combination of bag-of-words with expert features.

In order to train our classifiers, we had to create a feature matrix from the message and account data. We did this using 
Perl, Spark, and Lucene analyzers. The first step was to generate a dictionary sorted by document frequencies in descending 
order, mapping each feature (word or expert feature value) to an integer key. The second step was to create a sparse 
matrix representation of each document (in this case, spam or ham message) and the associated features. Spark provides a 
parallelized in-memory computation framework that dramatically reduced the time necessary to extract these features, 
build the dictionary and generate the feature matrix. 

We did not use all of the expert features available to us, due to time constraints. Likewise, for many expert features could 
have a wide range of values, which would expand the dictionary size considerably. In addition to the bag-of-words from the 
message text, we used the following features:
\begin{itemize}
\item Sex of sender
\item Age of sender
\item Age of recipient
\item Account lifetime
\item Month of birth 
\item Whether sender has friends (boolean)
\item Whether sender has photo(s) (boolean)
\item Profile country and IP-detected country match (boolean)
\item IP-detected country is Ghana, Nigeria, Senegal, Malaysia, Turkey, Gambia, Ivory Coast, Togo (``high-risk" countries) (boolean)
\end{itemize}

%classification:
%- implemented 3 models: naive bayes, linear regression, and SVM.
%- used scalanlp for SVM: Pegasos: Primal Estimated sub-GrAdient SOlver for SVM Extended with Wang, Crammer, Vucetic's work for Multiclass. The optimizer runs a stochastic subgradient descent on the primal objective using batches provided.
%- used scalanlp for naive bayes
%- used matlab for linear regression with ridge regularization.
%- also attempted logistics regression with scalanlp, but training result was buggy (always output 0 probably. perhaps a bug in scalanlp)
%
%- naive bayes is super fast. accuracy around 80%. when expert features are added, accuracy dropped.
%
%- SVM is slow, and doesn’t produce very good result. Suspect we don’t have enough training data (22k training examples, 5k features)
%
%- linear regression performed the best, with an AUC of 0.xx. When expert features are added, AUC went up to 0.xx.
%

\subsection{Na\"ive Bayes}

% Modified from Reynold's HW1  write-up
%We use a na\"ive Bayes model, in which the documents are generated according to the following process.  
%First, a class \(c\) is chosen from a probability distribution \(\Pr(c)\).
%Then, a document from class \(c\) is generated.
%We tried a multinomial model and a Bernoulli model to model this process.
%\begin{itemize}
%\item \emph{Multinomial model.}  A document length \(m_d\) is chosen and \(m_d\) words \(w_{d,1}, \dotsc, w_{d,m_d}\) are chosen independently from a distribution \(\Pr(w|c)\).
%\item \emph{Bernoulli model.}  For each word \(w\) in the vocabulary, \(w\) is added to the document with probability \(\Pr(w|c)\).
%\end{itemize}
%The distributions \(\Pr(c)\) and \(\Pr(w|c)\) are parameters of the model.
%
%
\subsection{Linear Regression}

Linear regression is a method to model the relationship between an output variable, 
$y$, and explanatory variables, $X$. The output variable is a linear sum of the explanatory 
variables multiplied by their corresponding coeffecients, represented by $\beta$. 
In our case, $y$ represents a message's label as spam or ham (1 or 0), 
while $X$ is the feature vector, a bag-of-words representation of the message text and expert features. 
This implies that $\beta$ represents the weight of a feature's correlation to whether 
the message is spam or not. This gives us the simple regression formula: $$y=X\beta$$

In this project, we performed ordinary least squares regression. By minimizing the sum of squared 
residuals, we can solve for the unkown parameter vector $\beta$ using a closed form solution. 
$$\beta=(X'X)^{-1}X'y$$ In order to ensure that the matrix $X'X$ is invertible, we perform ridge regression. 
In ridge regression, we add the identity matrix, $I$, scaled by a factor, $\lambda$, to 
produce the following equation. $$\beta=(X'X+\lambda I)^{-1}X'y$$

We calculate this $\beta$ for a training set of bag-of-words features along with their corresponding labels. 
We then evaluate our model $\beta$ on the remaining messages in order to perform 10-fold cross-validation. 
We repeat this process with the set that includes expert features.

All of our linear regression code was implemented in MATLAB because of its ease-of-use 
and optimizations on matrix multiplications. We read in the text-based feature dictionaries and feature matrix
representations created above, 

%We parse the messages from the text format produced by our cleaning script, using the dictionary generated earlier map words to integers, saving the feature data into two sparse matrices. The first matrix only includes the bag-of-words representation of the message body, while the second includes the expert features as well.  Matlab uses Compressed Sparse Column format, so we actually stored the matrix $X'$ because the dictionary is very sparse. 

%For each feature set, the actual matrix multiplications were done using the sparse $X$ matrix along with our $y$ vector and the ridge parameter. The multiplication $X'X$ takes up too much memory if we use the entire dictionary, so we chose to limit the dictionary size. We found that a dictionary of the 10,000 most frequent words gave sufficient features while also being computable using MATLAB's built-in sparse matrix multiplications. We also tried using a dictionary of 15,000 words, but only had a marginal improvement along with much slower execution. Using our 10,000 word dictionary, we calculated the performance of our matrix multiply to be 0.123 Gflops/s by dividing the number of non-zero elements in our matrices by the time to compute the multiplication. 
%
%
%Since we used ridge regression, we had the ability to tune the $\lambda$ parameter in order to get the best performance. As shown in figure \ref{fig:lambda}, we observed an AUC improvement of about .5\% between the default value of 1 and a tuned parameter of 1500. This is a fairly major difference since the AUC is at 93\% using the default value. Additionally, we also noticed a major difference in the top words when using better $\lambda$ values. Using the default value of 1, our top words included obscure words including ``\_c'', ``\_j'', ``donovan'', and ``matlock.'' When we used the tuned $\lambda$ value of 1500, the words were much more representative, as shown in table \ref{tab:words}.
%




\subsection{Logistic Regression}

We attempted to train the multiple logistic regression classifier implemented by the ScalaNLP package, but encountered a bug that prevented the model from being generated correctly. With the expectation that SVMs would outperform logistic regression, we decided to focus our efforts on tuning the SVM instead of pursuing logistic regression. 


\subsection{Support Vector Machines (SVMs)}

%intro to svms

Support Vector Machines are a classification method  \cite{boser}, \cite{cortesv95} 

% scalanlp svm 

% libsvm

LIBSVM is library for SVM training and classification in C++ and Java, with wrappers for Python and other languages. The package allows one to specify the type of kernel to use, kernel parameters ($\gamma$, $\rho$, $d$), as well as a penalty parameter ($C$). We used Perl to convert the sparse feature matrices generated for use with ScalaNLP to a format recognized by LIBSVM. We then used one of the package's tools to perform simple scaling on the data. 

Given the complexity of choosing kernels and parameters, the package includes a Python script that performs cross-validation to suggest appropriate $\gamma$ and $C$ values for the recommended radial basis function (RBF) kernel. The RBF kernel is a real-valued function such that: $$K(\mathbf{x}_i, \mathbf{x}_j) = (\gamma\mathbf{x}_i^\intercal\mathbf{x}_j+r)^d, \gamma>0$$
We ran this script on a subset of 2500 ham and 2500 spam messages to obtain $C = 8$ and $\gamma = 0.0078125$.

The LIBSVM scaling tool increases file size considerably for data sets with large feature matrices. This large file size, coupled with the computational resources required for SVM training on large data sets, prevented us from completing our analysis of the 

%\begin{tabular}{c|c|c}
%    \multicolumn{3}{c}{Unscaled: Data set size, Accuracy (bag-of-words), Accuracy (expert features)}
%    Data size & Accuracy, bag-of-words & Accuracy, expert features \\
%    \hline
%    2k & 0.0 & 0.0 \\
%    10k & 0.0 & 0.0 \\
%    22k & 0.0 & 0.0 \\ % note we chose 22k because 11k had the recipient info
%    50k  & 0.0 & 0.0 \\
%    100k  & 0.0 & 0.0 \\
%\end{tabular}
%
%
%\begin{tabular}{c|c|c}
%    \multicolumn{3}{c}{Scaled: Data set size, Accuracy (bag-of-words), Accuracy (expert features)}
%    Data size & Accuracy, bag-of-words & Accuracy, expert features \\
%    \hline
%    2k & 0.0 & 0.0 \\
%    10k & 0.0 & 0.0 \\
%    22k & 0.0 & 0.0 \\ % note we chose 22k because 11k had the recipient info
%\end{tabular}
%


\section{Results}

\subsection{Na\"ive Bayes}


\subsection{Linear Regression}

We tried a number of values for $\lambda$: 1, 10, 50, 100, 500, 1000, 1500, 3000. Figure \ref{fig:lambda}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/linear-ridge.pdf}
    \caption{AUC based on varying lambda values for Ridge regression.}
    \label{fig:lambda}
\end{figure}



%\begin{tabular}{c|c}
%    \multicolumn{2}{c}{Tokens most likely to be positive} \\
%    token & \(log(p_+ / p_-)\) \\
%    \hline
%    pos & 0.0 \\
%    pos2 & 0.1 \\
%\end{tabular}
%
%\begin{tabular}{c|c}
%    \multicolumn{2}{c}{Tokens most likely to be negative} \\
%    token & \(log(p_+ / p_-)\) \\
%    \hline
%    neg & 0.0 \\
%    neg2 & 0.1 \\
%\end{tabular}
%

\subsection{SVMs}

%\begin{tabular}{c|c}
%    \multicolumn{2}{c}{Tokens most likely to be positive} \\
%    token & \(log(p_+ / p_-)\) \\
%    \hline
%    pos & 0.0 \\
%    pos2 & 0.1 \\
%\end{tabular}
%
%\begin{tabular}{c|c}
%    \multicolumn{2}{c}{Tokens most likely to be negative} \\
%    token & \(log(p_+ / p_-)\) \\
%    \hline
%    neg & 0.0 \\
%    neg2 & 0.1 \\
%\end{tabular}
%

\section{Analysis}
% Vern wants site owner perspective on how to apply classifiers
From a site's perspective, ...

- plan to implement SVM, despite speed tradeoff 

\section{Future Work}

\subsection{Spam Grouping}

As a part of this project, we performed some qualitative analysis on spam and differentiated between the major categories of undesirable messages found on this SNS. However, we 

Characteristics of spam messages vary widely depending on the type of spam
we currently lump all spam into a single category; we plan to build on the classes we mentioned in section()
We are currently in the process of extending the moderation interface to include more specific tagging capabilities
%- certain classifiers might work better on certain types of spam
%- clustering to see how well these classes fit the spam types on the site


\subsection{Features}

In this project, we chose only a small subset of ``expert" features to use. However, we believe that it would be beneficial to train and evaluate the classifiers on a wider range of features. The features would include both the ones we extracted in this project but omitted from the classifier features matrices, as well as new features. Adding N-grams and message similarity tests seem particularly promising. N-grams would allow the classifier to take into account certain phrases \eg{``Western Union" or ``money transfer"}. 

We have also recently begun to collect browser fingerprints from users based on user agent and JavaScript-acquired browser plugin details, time zone, screen size, color depth, system fonts, keyboard layout and locale information. Subsequent classifiers could leverage this information either as a hash that effectively tags individual known spammers or in part, adding features based on time zone or keyboard layout.

It is also anecdotally clear from moderator log messages that spammers re-use their profile photos and these photos are often of celebrities, porn stars or simply stock photos. Hashing these images 


Server access logs, search logs and user viewing histories from InterPals, all of which are available to us, offer another avenue for future research. It is conceivable that certain spammers may have similar site usage patterns, in terms of HTTP request intervals, 


\subsection{Testing \& Evaluation}

- test SVM and other classifiers on newer data
- 

\section{Conclusion}

In this paper, we presented a brief overview of the breakdown of spam and scams on the InterPals web site. 
We then examined a number of statistics on both public and private data that showed that spam users, even when 
grouped together without regard to their specific angle of attack, share a number of characteristics that differentiate 
them from legitimate users. To simulate an approach 
These features allowed us to improve the accuracy of classifiers



figure \ref{fig:cprof}
figure \ref{fig:cip}
figure \ref{fig:drop}
figure \ref{fig:friends}
figure \ref{fig:roclin}
figure \ref{fig:photos}
figure \ref{fig:recipage}
figure \ref{fig:sendage}
figure \ref{fig:sendsex}
figure \ref{fig:roc-25}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/friends.pdf}
    \caption{Number of friends associated with sender account.}
    \label{fig:friends}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/linear-roc.pdf}
    \caption{Linear regression ROC curves for classifier using bag-of-words and bag-of-words with ``expert" features.}
    \label{fig:roclin}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/roc-25.pdf}
    \caption{LIBSVM ROC curves for classifier using bag-of-words and bag-of-words with ``expert" features on 50k messages.}
    \label{fig:roc-25}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/photos.pdf}
    \caption{Number of photos associated with sender account.}
    \label{fig:photos}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/sex.pdf}
    \caption{Sender sex as stated on profile.}
    \label{fig:sendsex}
\end{figure}




\bibliographystyle{abbrv}
\bibliography{paper}



\balancecolumns
\end{document}
